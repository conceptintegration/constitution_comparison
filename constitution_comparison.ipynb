{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Constitutions\n",
    "\n",
    "## Objective\n",
    "\n",
    "To explore methods for measuring the semantic similarity of pairs of constitutions.\n",
    "\n",
    "For example, are constitutions with neoliberal features more similar to one another than they are to constitutions with a more social deomocratic character.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__      = 'Roy Gardner'\n",
    "__copyright__   = 'Copyright 2023-2024, Roy and Sally Gardner'\n",
    "\n",
    "%run ./_library/packages.py\n",
    "%run ./_library/utilities.py\n",
    "%run ./_library/mapping.py\n",
    "%run ./_library/kde_pdf.py\n",
    "\n",
    "model_path = './model/'\n",
    "\n",
    "try:\n",
    "    n = len(model_dict)\n",
    "except:\n",
    "    model_dict = do_load(model_path,exclusion_list=[],verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF-based analysis\n",
    "\n",
    "Looks at the semantic similarity of pairs of constitutions.\n",
    "\n",
    "The analysis is based on a similarity matrix where the sections of all constitutions are in both rows and columns. Cell values contain the semantic similarity score for a pair of sections. \n",
    "\n",
    "For each constitution pairing:\n",
    "\n",
    "- For the first constitution in the pair collect the **row** indices from the similarity matrix correponding to the constitution's sections.\n",
    "- For the second constitution in the pair collect the **column** indices from the similarity matrix correponding to the constitution's sections.\n",
    "- Use the row and column indices to extract a sub-matrix from the similarity matrix.\n",
    "- Extract the maximum similarity values from the sub-matrix as follows:\n",
    "    - A first pass collects the row and column indices of the maximum value in rows.\n",
    "    - A second pass transposes the matrix to find the maximum values for original matrix columns (rows in the transposed matrix) not captured in the first pass.\n",
    "    - The rationale is that a row segment A may be maximally similar to a column segment B, but B might not be maximally similar to A\n",
    "- Use the maximum similarity values to generate a probability density function (PDF) using Kernel Density Estimation (KDE). Rationale:\n",
    "    - The more similar a pair of constitutions the longer and denser the right-hand tail of the distribution. This can be verified using test documents.\n",
    "- Integrate under the PDF in a given similarity score interval (default 0.8 - 1.0) to measure the likelihood that a pair of constitutions contain semantically similar sections.\n",
    "- Visualise the PDFs and the integral values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "documents_dict = model_dict['documents_dict']\n",
    "segments_dict = model_dict['segments_dict']\n",
    "encoded = model_dict['encoded_segments']\n",
    "segments_matrix = np.array(model_dict['segments_matrix'])\n",
    "\n",
    "doc_data = [(k,v['date']) for k,v in documents_dict.items()]\n",
    "doc_data = sorted(doc_data,key=lambda t:t[1])\n",
    "\n",
    "\n",
    "samples_list = []\n",
    "labels_list = []\n",
    "\n",
    "combs = combinations(doc_data,r=2)\n",
    "for c in combs:\n",
    "    row_doc = c[0][0]\n",
    "    col_doc = c[1][0]\n",
    "    #if not row_doc in ['Bolivia_2009','Ecuador_2021','Chile_2022D']:\n",
    "    #    continue\n",
    "    #if not col_doc in ['Bolivia_2009','Ecuador_2021','Chile_2022D']:\n",
    "    #    continue\n",
    "    \n",
    "    row_segment_ids = [k for k,_ in segments_dict.items() if k.split('/')[0] == row_doc and\\\n",
    "                             k in encoded]\n",
    "    row_segment_indices = [encoded.index(segment_id) for segment_id in row_segment_ids]\n",
    "    \n",
    "    col_segment_ids = [k for k,_ in segments_dict.items() if k.split('/')[0] == col_doc and\\\n",
    "                             k in encoded]\n",
    "    col_segment_indices = [encoded.index(segment_id) for segment_id in col_segment_ids]\n",
    "        \n",
    "    # Get the segments sub-matrix for the pair of documents\n",
    "    matrix = segments_matrix[np.ix_(row_segment_indices,col_segment_indices)]\n",
    "    samples_list.append(get_max_scores(matrix))\n",
    "    labels_list.append(str(c[0]) + ' - ' + str(c[1]))\n",
    "\n",
    "print()\n",
    "print('PDFs for pairs of constitutions from the set - see legend.')\n",
    "plot_pdfs(samples_list,labels_list,xlim=[0.5,1.0])\n",
    "\n",
    "limits = [0.7,1.0]\n",
    "\n",
    "integrals = get_pdf_integrals(samples_list,limits,sample_size=2)\n",
    "print()\n",
    "print('Relative magnitude of integrals in interval 0.8 - 1.0 for pairs of constitutions.')\n",
    "plot_pdf_integrals(integrals,labels_list,limits,'Similarity of constitution pair','Constitution pair',\\\n",
    "                   title_suffix='',figsize=(8,6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of sections over topics\n",
    "\n",
    "Given recent analysis of ontology workbench outputs, and segments-as-topics, there are questions around the value of the following analysis.\n",
    "\n",
    "In this section the the distribution of sections over CCP topics is measured for each of the four constitutions at a semantic threshold value. In other words, we find the number of at or above threshold sections for each topic.\n",
    "\n",
    "Pearsons R is used to measure the correlation between pairs of constitutions (see below the graph). All correlations are high and highly significant, The pairs with the highest correlations are:\n",
    "\n",
    "- The in-force constitution and the Bachelet draft\n",
    "- The in-force constitution and the 2023 draft\n",
    "\n",
    "This is consistent with the PDF-based analysis above.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = model_dict['encoded_segments']\n",
    "doc_ids = set([sid.split('/')[0] for sid in encoded])\n",
    "\n",
    "threshold = 0.66\n",
    "\n",
    "topic_ids = model_dict['encoded_topics']\n",
    "    \n",
    "topic_dict = {}\n",
    "for did in doc_ids:\n",
    "    y = []\n",
    "    for i,row in enumerate(model_dict['topic_segment_matrix']):\n",
    "        y.append(len([j for j,v in enumerate(row) if v >= threshold and\\\n",
    "                                          encoded[j].split('/')[0]==did]))\n",
    "    topic_dict[did] = y\n",
    "\n",
    "x = range(0,len(topic_ids))\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "for did,y in topic_dict.items():\n",
    "    plt.bar(x,y,alpha=0.4)\n",
    "    plt.scatter(x,y,alpha=0.4,label=did)\n",
    "plt.xticks(fontsize='xx-large')\n",
    "plt.yticks(fontsize='xx-large')\n",
    "plt.xlabel('Topic index',fontsize='xx-large')\n",
    "plt.xlabel('Number of sections',fontsize='xx-large')\n",
    "plt.legend(fontsize='xx-large')\n",
    "plt.title('Semantic similarity threshold = 0.66')\n",
    "plt.show()\n",
    "\n",
    "print('Chile_2021-Chile_2018D',stats.pearsonr(topic_dict['Chile_2021'],topic_dict['Chile_2018D']))\n",
    "print('Chile_2021-Chile_2022D',stats.pearsonr(topic_dict['Chile_2021'],topic_dict['Chile_2022D']))\n",
    "print('Chile_2021-Chile_2023DD',stats.pearsonr(topic_dict['Chile_2021'],topic_dict['Chile_2023DD']))\n",
    "print('Chile_2018D-Chile_2022D',stats.pearsonr(topic_dict['Chile_2018D'],topic_dict['Chile_2022D']))\n",
    "print('Chile_2018D-Chile_2023DD',stats.pearsonr(topic_dict['Chile_2018D'],topic_dict['Chile_2023DD']))\n",
    "print('Chile_2022D-Chile_2023DD',stats.pearsonr(topic_dict['Chile_2022D'],topic_dict['Chile_2023DD']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic scoring topics from the distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "\n",
    "print()\n",
    "\n",
    "order = ['Chile_2021','Chile_2018D','Chile_2022D','Chile_2023DD','Bolivia_2009','Ecuador_2021']\n",
    "\n",
    "for k in order:\n",
    "    print(k)\n",
    "\n",
    "    p = find_peaks(topic_dict[k],height=10)\n",
    "    z = list(zip(list(p[0]),list(p[1]['peak_heights'])))\n",
    "    z = sorted(z,key=lambda t:t[1],reverse=True)\n",
    "    for t in z[0:6]:\n",
    "        print('\\t',topic_ids[t[0]])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
